<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),n=e=>e.innerText,t=e=>{let n=e.split(" "),t=n.slice(0,-1).join(" ");return[n.at(-1),t]},i=Array.from(document.getElementsByClassName("author")).map(n).map(e).map(t),o=i[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(n).map(e),"April 28, 2025"),r="AI810 Blog Post (20205266)",l="This blogpost reviews two ICLR 2025 papers, one on RL-guided DNA sequence design (TACO), and another on a unified generative modeling framework based on Markov processes (Generator Matching). Though from different domains, both highlight how structured priors and guided dynamics improve generative outcomes. Their pairing reveals shared insights relevant to reinforcement learning and LLM-based agents, especially in offline optimization, reward shaping, and multi-agent compositionality. For researchers in intelligent agent design, these works offer complementary views on controllable generation\u2014whether in biology, vision, or language.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),n=`\n@inproceedings{${(o+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=n}{let e=i.map(e=>e[0]),n=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=n}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>AI810 Blog Post (20205266) | AI810_Geometric Deep Learning_test</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This blogpost reviews two ICLR 2025 papers, one on RL-guided DNA sequence design (TACO), and another on a unified generative modeling framework based on Markov processes (Generator Matching). Though from different domains, both highlight how structured priors and guided dynamics improve generative outcomes. Their pairing reveals shared insights relevant to reinforcement learning and LLM-based agents, especially in offline optimization, reward shaping, and multi-agent compositionality. For researchers in intelligent agent design, these works offer complementary views on controllable generation—whether in biology, vision, or language."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/marl/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/marl/assets/css/main.css"> <link rel="canonical" href="https://yongsik-lee.github.io/marl/blog/20205266/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/marl/assets/js/theme.js"></script> <script src="/marl/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/marl/assets/js/distillpub/template.v2.js"></script> <script src="/marl/assets/js/distillpub/transforms.v2.js"></script> <script src="/marl/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "AI810 Blog Post (20205266)",
      "description": "This blogpost reviews two ICLR 2025 papers, one on RL-guided DNA sequence design (TACO), and another on a unified generative modeling framework based on Markov processes (Generator Matching). Though from different domains, both highlight how structured priors and guided dynamics improve generative outcomes. Their pairing reveals shared insights relevant to reinforcement learning and LLM-based agents, especially in offline optimization, reward shaping, and multi-agent compositionality. For researchers in intelligent agent design, these works offer complementary views on controllable generation—whether in biology, vision, or language.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Yongsik Lee",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/marl/">AI810_Geometric Deep Learning_test</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/marl/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/marl/blog/index.html">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>AI810 Blog Post (20205266)</h1> <p>This blogpost reviews two ICLR 2025 papers, one on RL-guided DNA sequence design (TACO), and another on a unified generative modeling framework based on Markov processes (Generator Matching). Though from different domains, both highlight how structured priors and guided dynamics improve generative outcomes. Their pairing reveals shared insights relevant to reinforcement learning and LLM-based agents, especially in offline optimization, reward shaping, and multi-agent compositionality. For researchers in intelligent agent design, these works offer complementary views on controllable generation—whether in biology, vision, or language.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#regulatory-dna-sequence-design-with-reinforcement-learning">Regulatory DNA Sequence Design with Reinforcement Learning</a></div> <ul> <li><a href="#biological-bacgkround">Biological Bacgkround</a></li> <li><a href="#overview">Overview</a></li> <li><a href="#method">Method</a></li> <li><a href="#results-analysis">Results &amp; Analysis</a></li> <li><a href="#personal-take-commentary">Personal Take &amp; Commentary</a></li> </ul> <div><a href="#generator-matching-generative-modeling-with-arbitrary-markov-processes">Generator Matching: Generative Modeling with Arbitrary Markov Processes</a></div> <ul> <li><a href="#overview">Overview</a></li> <li><a href="#method">Method</a></li> <li><a href="#results-discussion">Results &amp; Discussion</a></li> <li><a href="#personal-take-commentary">Personal Take &amp; Commentary</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>This review pairs two thematically distinct yet intellectually adjacent papers: “Regulatory DNA Sequence Design with Reinforcement Learning” and “Generator Matching: Generative Modeling with Arbitrary Markov Processes.” While one operates in the domain of biological sequence design and the other in foundational generative modeling, both tackle a common question at the heart of intelligent system design: how can we guide generative processes using structured priors and dynamic feedback?</p> <p>The first paper explores how reinforcement learning—augmented with biological knowledge—can improve the optimization of cis-regulatory DNA sequences, a crucial problem in synthetic biology. The second paper introduces a unifying mathematical framework for generative modeling through the lens of parameterized Markov processes, offering a flexible alternative to existing paradigms like diffusion and flow models.</p> <p>From the vantage point of RL and LLM-based agents, both works offer rich insights. TACO (Paper 1) exemplifies reward shaping and policy optimization in a constrained generative environment, while Generator Matching (Paper 2) provides a theoretical scaffold for modeling and composing stochastic transitions—akin to environment dynamics in model-based RL or planning behaviors in complex multi-agent systems. Despite addressing different domains, both papers converge on a shared ambition: to systematically construct or adapt generative mechanisms toward target outcomes using theoretically grounded tools.</p> <h2 id="regulatory-dna-sequence-design-with-reinforcement-learning-">Regulatory DNA Sequence Design with Reinforcement Learning <d-cite key="yang2025regulatory"></d-cite> </h2> <h3 id="-biological-background">📌 Biological Background</h3> <p>As we come from an AI background, I first provide a brief summary of relevant biological concepts appearing in this paper, based on my own understanding with external sources.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-biology-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-biology-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-biology-1400.webp"></source> <img src="/marl/assets/img/2025-04-28-20205266/TACO-biology.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Conceptual flow of gene expression control. </div> <ul> <li> <strong>DNA</strong>: a sequence of nucleotides (A, T, C, and G) that encodes genetic information. <ul> <li> <strong>Gene</strong>: a sub-sequence of DNA. Genes are not always active. <strong>Gene expression</strong> is the process that encodes a gene’s information into proteins.</li> <li> <strong>Cis-regulatory elements (CRE)</strong>: a non-coding sub-sequence of DNA that acts as on/off switch to control gene expression. A <strong>promoter</strong> determines when and where a gene is activated, and an <strong>enhancer</strong> boosts the level of gene expression. The ability of a CRE to modulate gene expression is referred to as its <strong>fitness</strong>. <ul> <li> <strong>Transcription Factor Binding Sites (TFBS)</strong>: a short sequence motif within a CRE.</li> </ul> </li> </ul> </li> <li> <strong>Transcription Factor (TF)</strong>: a protein that recognizes and binds to a specific TFBS. This binding influences a CRE’s regulation of gene expression. An <strong>activator</strong> TF promotes gene transcription and expression, while a <strong>repressor</strong> TF hinders them.</li> </ul> <p>In summary, a TF binding to a CRE via its TFBS modulates gene expression and each gene is regulated by distinct TFs and CREs.</p> <h3 id="-overview">📌 Overview</h3> <h4 id="motivation">Motivation</h4> <p>CREs play an essential role in regulating gene expression in a cell-type-specific manner. While millions of putative CREs have been identified over the past decade, most are naturally evolved and cover only a small region of the possible sequence space. Therefore, the design of synthetic CREs with <em>desired fitness</em> is a promising direction, with broad applications across diverse domains.</p> <p>The design of high-fitness CREs has primarily relied on <em>directed evolution</em>, which iteratively mutates and selects sequences in wet-lab settings. More recently, <em>fitness prediction models</em> have been utilized as reward models to guide CRE optimization. However, current methods suffer from two limiations:</p> <ul> <li>Although the sequence space is large, they rely on local modifcation of existing or random sequences with iterative optimization, resulting in <em>local optima</em> and <em>low diversity</em>.</li> <li>They generally do not utilize <em>biological prior knowledge</em>.</li> </ul> <h4 id="key-contributions">Key Contributions</h4> <p>This paper proposes <strong>TACO</strong> (<strong>T</strong>FBS-<strong>A</strong>ware <strong>C</strong>is-regulatory element <strong>O</strong>ptimization), a RL fine-tuning method for a pre-trained autoregressive (AR) DNA model incorporating biological priors of TFBS information to improve CRE optimization. The key contributions are:</p> <ul> <li> <strong>RL Fine-tuning for Pre-trained AR DNA Generative Models</strong>: The suggested paradigm enables the generation of sequences with significantly higher diversity while also exploring those with superior functional performance.</li> <li> <strong>Biologically-informed Prior Guided TFBS Reward</strong>: The authors discover that using only TFBS frequency features of a CRE sequence can achieve high performance on CRE fitness prediction tasks. Moreover, the potential contribution of each TFBS is inferred via SHAP value and implemented as additional rewards.</li> <li> <strong>Generation of high fitness and diversity Cell-type specific CREs</strong>: TACO is evaluated under different optimization settings (active learning and offline model-based optimization) on real-world datasets and demostrated its effectiveness.</li> </ul> <h3 id="-method">📌 Method</h3> <h4 id="problem-formulation">Problem Formulation</h4> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method-1400.webp"></source> <img src="/marl/assets/img/2025-04-28-20205266/TACO-method.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Overview of TACO, illustrating AR generation of a DNA sequence (BOS represents the beginning of the sequence). </div> <p>A DNA sequence \(X = ( x_1, \cdots, x_L )\) is defined as a sequence of necleotides, where \(x_i \in \{A, C, G, T \}\) represents the nucleotide at the \(i\)-th position. The sequence has length \(L\). A large-scale dataset of CRE sequences with fitness measurements, \(D = \{ (X_1, f (X_1)), \cdots, (X_N, f (X_N)) \}\), is available. \(D_\text{low}\) is a subset of low-fitness sequences. In RL framework, the sequence generation is formulated as a <em>Markov Decision Process (MDP)</em>:</p> <ul> <li> <strong>State</strong>: \(s_i\), a partially generated DNA sequences up to time step \(i\)</li> <li> <strong>Action</strong>: \(a_i \in \{A, C, G, T \}\), the next nucleotide at position \(i\)</li> <li> <strong>Policy</strong>: \(\pi_\theta\), the AR generative model</li> <li> <strong>Reward</strong>: \(r(s_{i-1}, a_i) = \begin{cases} r_{\text{fitness}}, &amp; \text{if } i = L \\ r_{\text{TFBS}}(t), &amp; \text{if } a_i \text{ results in a TFBS } t \in T \\ 0, &amp; \text{otherwise} \end{cases}\)</li> </ul> <p>The generation process is illustrated in the above figure. The process terminates when the sequence length reaches $L$ and \(r_{\text{fitness}}\) is given by the reward model. Whenever a TFBS \(T = \{t_1, t_2, \cdots, t_n\}\) is identified, a positive (or negative) reward \(r_{\text{TFBS}}(t)\) is given for generating activating (or repressive) TFBS.</p> <h4 id="step-1-pre-training-cre-specific-ar-model">Step 1: Pre-training CRE-specific AR Model</h4> <p>In the pre-training stage, HyenaDNA <d-cite key="nguyen2023hyenadna"></d-cite> is adapted for the AR model by continual training on \(D_{\text{low}}\). As HyenaDNA is trained on the entire human genome, continual pre-training is performed for CRE-specific regulatory patterns. Furthermore, pre-training on \(D_{\text{low}}\) helps the policy generate sequences that resemble the true CRE distribution <d-cite key="jin2020multi"></d-cite><d-cite key="chen2021molecule"></d-cite>, offering a good starting point for RL fine-tuning. The objective is to minimize:</p> \[\min_{\theta} \mathbb{E}_{x \sim D_{\text{low}}} \left[ \sum_{i=1}^{L} -\log \pi_{\theta}(a_i \mid a_1, \cdots, a_{i-1}) \right]\] <h4 id="step-2-rl-fine-tuning-for-ar-dna-models">Step 2: RL Fine-tuning for AR DNA Models</h4> <p>With the aforementioned MDP formulation, the objective in RL fine-tuning stage is to maximize the expected cumulative rewards:</p> \[\max_{\theta} J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{i=1}^{L} r(s_{i-1}, a_i) \right]\] <p>REINFORCE <d-cite key="williams1992simple"></d-cite> is used to train the policy, and a hill climbing replay buffer and entropy regularization are utilized as auxiliary techniques following prior works <d-cite key="blaschke2020reinvent"></d-cite><d-cite key="ghugaresearching"></d-cite> to balance exploration and exploitation, thus improving performance.</p> <h4 id="inference-of-tfbs-regulatory-roles-integrating-biological-prior">Inference of TFBS Regulatory Roles (Integrating Biological Prior)</h4> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/marl/assets/img/2025-04-28-20205266/TACO-method2-1400.webp"></source> <img src="/marl/assets/img/2025-04-28-20205266/TACO-method2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Inference of TFBS Reward $r_{\text{TFBS}}(t)$ </div> <p>Firstly, construct the feature vector \(h(X) = [h_1(X), h_2(X), \cdots, h_n(X)]\) where \(h_j(X)\) denotes the frequency of the TFBS \(t_j\) in sequence \(X\). LightGBM <d-cite key="ke2017lightgbm"></d-cite>, a decision-tree model, is trained to predict CRE fitness with this feature.</p> <p>Then the contribution of each TFBS frequency feature \(h_j(X)\) to the fitness prediction of the LightGBM model is inferred using SHAP values <d-cite key="lundberg2017unified"></d-cite> <d-footnote>SHAP values are a theoretically grounded method to estimate the contribution of each feature to the prediction of a model.</d-footnote>. The SHAP value \(\phi_j(X)\) for \(j\)-th TFBS \(t_j\) in sequence \(X\) is:</p> \[\phi_j(X) = \sum_{S \subseteq \{1, \dots, n\} \setminus \{j\}} \frac{|S|!(n - |S| - 1)!}{n!} \left( \hat{f}(S \cup \{j\}) - \hat{f}(S) \right)\] <p>where \(S\) is a subset of features not containing \(j\) and \(\hat{f}\) is the model prediction. Then the TFBS reward \(r_{\text{TFBS}}(t)\) is computed as:</p> \[r_{\text{TFBS}}(t) = \begin{cases} \alpha \cdot \mu_\phi(t), &amp; \text{if } p\text{-value} &lt; 0.05 \\ 0, &amp; \text{otherwise} \end{cases}\] <p>where \(\alpha\) is a hyperparameter and \(\mu_\phi(t)\) is the mean SHAP value of TFBS \(t\) across the dataset. <d-footnote> By assigning rewards only when p-value is less than 0.05, only statistically significant TFBSs contribute to the reawrds. </d-footnote> These rewards are incorporated into the RL fine-tuning to encourage activator TFBSs and discourage repressive TFBSs.</p> <h3 id="-results--analysis">📌 Results &amp; Analysis</h3> <p>The experiments are conducted on two datasets: <strong>yeast promoter</strong>, which includes two types of growth media (<em>complex</em> and <em>defined</em>) with DNA sequence length 80, and <strong>human enhancer</strong>, which consists of three cell lines (<em>HepG2</em>, <em>K562</em>, and <em>SK-N-SH</em>) with sequence length 200. MPRAs were employed to obtain all paired CRE sequences and their corresponding fitness measurements.</p> <p>There evaluation metrics are used. <em>Top</em> is the mean fitness of highest-performaing 16 sequences from the optimized set \(\Chi^* = \{X_1, \ldots, X_K\}\) of 256 sequences. <d-footnote> In each optimization round, $K=256$ sequences are generated. </d-footnote> Both <em>Medium</em> and <em>Diversity</em> are computed using the highest-performing 128 sequences from the set of 256 sequences. <em>Medium</em> is the median fitness among 128 sequences, and <em>Diversity</em> is the median pairwise distance between all paris within 128 sequences.</p> <h4 id="experimental-summary">Experimental Summary</h4> <p>The evaluation covers two domains:</p> <ul> <li> <strong>Yeast promoters</strong> (short sequences, simpler regulatory grammar).</li> <li> <strong>Human enhancers</strong> (longer, cell-type-specific complexity).</li> </ul> <p>Settings:</p> <ul> <li> <strong>Active Learning</strong>: The oracle is visible during training.</li> <li> <strong>Offline Model-Based Optimization (MBO)</strong>: The oracle is hidden; optimization relies on a surrogate.</li> </ul> <p>Baselines include BO, CMAES, AdaLead, PEX, and DNARL.</p> <h4 id="empirical-analysis">Empirical Analysis</h4> <ul> <li> <strong>Yeast Results</strong>: All models achieved max fitness, but <strong>TACO surpassed all in diversity</strong>, suggesting better exploration.</li> <li> <strong>Human Enhancers</strong>: <ul> <li> <strong>TACO maintained the highest diversity</strong> while achieving comparable or superior fitness, particularly in the K562 and SK-N-SH cell lines.</li> <li> <strong>PEX</strong> had high fitness but low diversity.</li> <li> <strong>AdaLead</strong> was fast but prone to premature convergence.</li> </ul> </li> <li> <strong>Offline MBO</strong>: TACO preserved its edge in diversity and achieved strong performance without oracle access.</li> </ul> <p>Notably, TACO’s use of <strong>biologically informed rewards</strong> enabled it to escape local optima and avoid overfitting to surrogate models, a known failure mode in offline optimization.</p> <h4 id="strengths">Strengths</h4> <ul> <li><strong>Innovative use of TFBS-informed rewards</strong></li> <li> <strong>High diversity and fitness</strong>—critical in real applications</li> <li> <strong>Robust performance</strong> across domains and training settings</li> <li> <strong>Strong theoretical grounding</strong> in MDP and interpretable feature attribution</li> </ul> <h4 id="limitations">Limitations</h4> <ul> <li> <strong>Static TFBS vocabulary</strong>: Based on a fixed database; dynamic motif discovery is unexplored.</li> <li> <strong>No modeling of TF interactions or orientation</strong>: Limits biological realism.</li> <li> <strong>Fitness oracle is assumed accurate</strong>: Real-world biological validation is absent.</li> <li> <strong>Generality across organisms</strong>: Not yet demonstrated outside yeast/human.</li> </ul> <h3 id="personal-take--commentary">Personal Take &amp; Commentary</h3> <h4 id="interpretation-from-my-research-background">Interpretation from My Research Background</h4> <p>As a researcher in <strong>RL and multi-agent systems</strong>, particularly with recent work in <strong>LLM-based agents</strong>, this paper feels like a well-integrated application of RL to structured sequence design. The use of a generative AR model as a policy echoes <strong>autoregressive planning</strong> in agents, and the reward shaping via TFBS knowledge mirrors <strong>value shaping in multi-agent learning</strong> to incorporate domain priors.</p> <p>Moreover, the method’s ability to balance <strong>exploration (diversity)</strong> and <strong>exploitation (fitness)</strong> under surrogate uncertainty aligns with ideas from <strong>conservative RL</strong> and <strong>offline policy optimization</strong> in LLM agents. The idea of inferring “reward models” from interpretable features also shares philosophical ground with reward learning in autonomous agents.</p> <h4 id="implications-for-future-research">Implications for Future Research</h4> <ul> <li> <strong>Cross-domain generalization</strong>: Could TACO-style methods transfer to protein engineering or RNA design with appropriate priors?</li> <li> <strong>Multi-objective optimization</strong>: Incorporating trade-offs (e.g., tissue specificity vs. stability) could benefit from <strong>Pareto front-based RL</strong>.</li> <li> <strong>Hierarchical action spaces</strong>: DNA motifs are naturally hierarchical. Could <strong>options or temporally extended actions</strong> improve sample efficiency?</li> <li> <strong>Language model parallels</strong>: Applying chain-of-thought prompting or instruction tuning to DNA generation is an intriguing idea.</li> </ul> <h4 id="critical-commentary">Critical Commentary</h4> <ul> <li>While TACO clearly advances the state of the art, it remains dependent on oracle quality. Exploring <strong>uncertainty-aware RL</strong> or <strong>model-based planning</strong> using ensembles could mitigate this.</li> <li>The <strong>lack of experimental wet-lab validation</strong> leaves the practical impact of the method speculative, though this is a systemic issue in computational biology.</li> <li>The method could benefit from <strong>probabilistic modeling of biological constraints</strong>, perhaps by fusing the AR model with a <strong>variational framework</strong>.</li> </ul> <h2 id="-tldr">🌟 TL;DR</h2> <p><strong>TACO is a biologically-aware RL method for DNA sequence design that integrates generative modeling and interpretable reward shaping.</strong> It sets a new benchmark in CRE optimization by balancing fitness and diversity—two goals often in conflict—and represents a compelling bridge between computational biology and reinforcement learning. For RL researchers, this is a refreshing demonstration of classic ideas in a novel and high-impact domain.</p> <h2 id="generator-matching-generative-modeling-with-arbitrary-markov-processes-">Generator Matching: Generative Modeling with Arbitrary Markov Processes <d-cite key="holderrieth2025generator"></d-cite> </h2> <h3 id="introduction-1">Introduction</h3> <h4 id="motivation-1">Motivation</h4> <p>In recent years, generative modeling has been dominated by paradigms such as <strong>VAEs</strong>, <strong>GANs</strong>, <strong>Diffusion Models</strong>, and <strong>Flow Models</strong>. Despite their differences, these models share a structural similarity: they operate over <strong>Markovian transformations of probability distributions</strong>, incrementally transforming simple priors into complex data distributions.</p> <p>This paper introduces <strong>Generator Matching (GM)</strong>, a framework that unifies these approaches under the abstraction of <strong>parameterized Markov generators</strong>. This unification not only connects flow-based and diffusion models but also reveals previously unexplored classes of generative processes, such as jump processes, and provides a principled method for combining them.</p> <h4 id="problem-statement">Problem Statement</h4> <p>How can we construct a scalable, flexible, and modality-agnostic generative modeling framework grounded in the theory of <strong>Markov processes</strong> that both unifies existing methods and enables the development of new ones?</p> <h4 id="key-contributions-1">Key Contributions</h4> <ul> <li> <strong>Unified Framework</strong>: Proposes <strong>Generator Matching</strong>, generalizing flow matching, diffusion models, and discrete diffusion models.</li> <li> <strong>Universal Characterization</strong>: Provides a comprehensive characterization of Markov process generators in Euclidean and discrete spaces.</li> <li> <strong>New Model Class</strong>: Introduces <strong>jump models</strong> in $\mathbb{R}^d$ as a novel generative modeling approach.</li> <li> <strong>Model Combinations</strong>: Introduces <strong>Markov superpositions</strong> and principled multimodal model composition.</li> <li> <strong>Empirical Validation</strong>: Demonstrates competitive performance in image and multimodal protein generation.</li> </ul> <h3 id="method">Method</h3> <h3 id="core-model-architecture--theoretical-framework">Core Model Architecture / Theoretical Framework</h3> <p>The GM framework is based on the <strong>infinitesimal generator</strong> $L_t$ of a Markov process $(X_t)_{t \in [0,1]}$, satisfying the <strong>Kolmogorov Forward Equation (KFE)</strong>:</p> \[\partial_t \mathbb{E}_{x \sim p_t}[f(x)] = \mathbb{E}_{x \sim p_t}[L_t f(x)]\] <p>The approach involves:</p> <ul> <li> <table> <tbody> <tr> <td>Selecting a <strong>conditional probability path</strong> $p_t(\cdot</td> <td>z)$ that interpolates between a simple prior $p_0$ and a delta distribution at data point $z$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Deriving a <strong>conditional generator</strong> $L^z_t$ satisfying the KFE for $p_t(\cdot</td> <td>z)$.</td> </tr> </tbody> </table> </li> <li>Aggregating these to obtain the <strong>marginal generator</strong>:</li> </ul> \[L_t f(x) = \mathbb{E}_{z \sim p_{1|t}(\cdot | x)}[L^z_t f(x)]\] <p>Training involves minimizing a <strong>conditional Generator Matching (CGM) loss</strong> using a <strong>Bregman divergence</strong> between the ground truth and parameterized generator outputs.</p> <p>In Euclidean space, the generator can be expressed as:</p> \[L_t f(x) = \nabla f(x)^T u_t(x) + \frac{1}{2} \text{Tr}\left[\nabla^2 f(x) \cdot \sigma_t^2(x)\right] + \int [f(y) - f(x)] Q_t(dy; x)\] <p>This formulation unifies:</p> <ul> <li>Flows: via drift $u_t$</li> <li>Diffusions: via noise $\sigma_t$</li> <li>Jumps: via rate kernel $Q_t$</li> </ul> <h3 id="results">Results</h3> <h4 id="experimental-summary-1">Experimental Summary</h4> <p>Experiments were conducted on:</p> <ul> <li> <strong>Image generation</strong> (CIFAR-10, ImageNet32)</li> <li> <strong>Multimodal protein design</strong> (combining structure and sequence modalities)</li> </ul> <p>Key findings:</p> <ul> <li> <strong>Jump models</strong>, a novel class, exhibit reasonable generative capacity.</li> <li> <strong>Markov superpositions</strong> (e.g., combining flow and jump models) enhance FID scores.</li> <li> <strong>Multimodal extensions</strong> demonstrate state-of-the-art diversity in protein generation.</li> </ul> <h4 id="empirical-analysis-1">Empirical Analysis</h4> <h5 id="image-generation">Image Generation</h5> <ul> <li> <strong>Jump models</strong> perform adequately on CIFAR-10 and ImageNet32 but do not surpass finely tuned flow/diffusion models.</li> <li>Combining <strong>jump + flow</strong> via superpositions yields improved FID scores compared to individual models.</li> <li>Hybrid sampling (Euler for jumps, 2nd order ODE for flows) achieves the best results.</li> </ul> <table> <thead> <tr> <th>Model</th> <th>CIFAR-10 (FID)</th> <th>ImageNet32 (FID)</th> </tr> </thead> <tbody> <tr> <td>Flow (Euler)</td> <td>2.94</td> <td>4.58</td> </tr> <tr> <td>Jump (Euler)</td> <td>4.23</td> <td>7.66</td> </tr> <tr> <td>Jump + Flow (Mixed)</td> <td><strong>2.36</strong></td> <td><strong>3.33</strong></td> </tr> </tbody> </table> <h5 id="protein-generation">Protein Generation</h5> <ul> <li>Integration of SO(3) jump models into MultiFlow improves <strong>diversity</strong> and <strong>novelty</strong>.</li> <li>GM enables seamless <strong>multimodal state space modeling</strong>.</li> </ul> <h4 id="strengths-1">Strengths</h4> <ul> <li> <strong>Theoretical Depth</strong>: Grounded in stochastic process theory, offering solid derivations and generality.</li> <li> <strong>Modality-Agnostic</strong>: Applicable across $\mathbb{R}^d$, discrete spaces, and manifolds (e.g., SO(3)).</li> <li> <strong>Unified Perspective</strong>: Provides a principled framework encompassing various generative paradigms.</li> <li> <strong>Innovative Potential</strong>: Introduces new design principles, particularly in combining generative models across modalities and mechanisms (flow, diffusion, jumps).</li> <li> <strong>Empirical Validation</strong>: Demonstrates the practical viability of the framework through experiments on image and protein generation tasks.</li> </ul> <h4 id="limitations-1">Limitations</h4> <ul> <li> <strong>Sampling Efficiency</strong>: Sampling methods for jump models are less optimized compared to flows; only Euler methods are currently available.</li> <li> <strong>Training Stability</strong>: The impact of jump-induced discontinuities on training stability is not thoroughly explored.</li> <li> <strong>Computational Overhead</strong>: Learning and simulating general generators, especially those involving integration over jump measures, can be computationally intensive.</li> <li> <strong>Comparative Analysis</strong>: Limited comparison to autoregressive models, which are prevalent in many multimodal tasks (e.g., LLMs for text).</li> </ul> <h3 id="personal-take--commentary-1">Personal Take &amp; Commentary</h3> <h4 id="interpretation-from-my-rl--multi-agent--llm-agent-background">Interpretation from My RL / Multi-Agent / LLM Agent Background</h4> <p>From the perspective of <strong>Reinforcement Learning</strong> and <strong>LLM-based agents</strong>, the concept of parameterizing <strong>Markov generators</strong> parallels the modeling of <strong>transition dynamics</strong> in model-based RL. Just as structured priors in environment modeling can enhance planning, Generator Matching offers a structured approach to define transition dynamics of distributions toward a data manifold.</p> <p>In <strong>multi-agent systems</strong>, GM’s framework for <strong>composing models</strong> (Markov superpositions, multimodal joints) suggests intriguing possibilities: combining agents with different generative priors or handling discrete-continuous hybrid state-action spaces.</p> <p>Furthermore, the <strong>Bregman divergence-based CGM loss</strong> generalizes many losses used in score-based models and could be adaptable to RL-style divergence regularization objectives.</p> <h4 id="implications-for-future-research-1">Implications for Future Research</h4> <ol> <li> <strong>RL Applications</strong>: Exploring GM-inspired generators for learning flexible environment models in stochastic settings.</li> <li> <strong>LLM Agents</strong>: Investigating the discrete diffusion perspective of language generation as an alternative to autoregressive decoding.</li> <li> <strong>Control as Generation</strong>: Viewing control policies as generators transforming initial states to desired outcomes, potentially inspiring new training objectives for hierarchical policies.</li> </ol> <h4 id="critical-commentary-1">Critical Commentary</h4> <ul> <li>The <strong>mathematical rigor</strong> is commendable, but practical challenges, especially in efficient sampling and scalable implementation, need further exploration. Real-world applications require robust inference schemes for jump models.</li> <li>The treatment of <strong>multimodality</strong> is theoretically sound but may oversimplify practical complexities, as real tasks involve intricate conditional dependencies.</li> <li> <strong>Ablation studies</strong> on the expressive benefits of modeling diffusion versus jump components separately would provide deeper insights.</li> <li> <table> <tbody> <tr> <td>The assumption of access to the posterior $p_{1</td> <td>t}(z</td> <td>x)$ may pose challenges in complex domains, as approximations could become bottlenecks.</td> </tr> </tbody> </table> </li> </ul> <h2 id="-tldr-1">📌 TL;DR</h2> <p><strong>Generator Matching</strong> presents a mathematically elegant and theoretically rich foundation for generative modeling using arbitrary Markov processes. It unifies existing methods under a cohesive framework and introduces powerful new design principles, particularly in combining and composing generative models across modalities and mechanisms. While practical challenges remain, especially concerning jump-based sampling and training, the framework opens exciting avenues for applications in RL, LLMs, and beyond.</p> <h2 id="conclusion">Conclusion</h2> <p>Both papers, though rooted in different problem domains, demonstrate how structured guidance—whether in the form of reward functions or stochastic generators—can profoundly influence generative performance. TACO operationalizes reinforcement learning with interpretable, biologically grounded reward shaping, showing how domain priors can lead to both better optimization and greater output diversity. Generator Matching, on the other hand, reimagines generative modeling itself as a problem of parameterizing transitions in a Markovian space, offering a versatile and modular approach to synthesizing data across modalities.</p> <p>For researchers in reinforcement learning and LLM-based agents, these works offer complementary lessons. TACO echoes challenges in offline RL and exploration under uncertainty—issues also central to agent design in partially observable or data-scarce environments. Generator Matching suggests a formalism for model-based generation and planning, with intriguing implications for LLMs as stochastic transformers of information states, potentially inspiring new agent design strategies based on generator composition.</p> <p>Finally, from a multi-agent systems perspective, both methods hint at future directions: in TACO, one could envision population-based or competitive optimization of DNA sequences; in Generator Matching, the compositionality of generators suggests a modular approach to modeling heterogeneous agent behaviors or hybrid continuous-discrete action spaces.</p> <p>Together, these papers underscore a powerful trend: bridging generative modeling with policy design, whether for biology, vision, or intelligent agents. Their relevance spans from synthetic cells to synthetic cognition—united by a shared focus on controlled, high-fidelity generation in structured environments.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/marl/assets/bibliography/2025-04-28-20205266.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>